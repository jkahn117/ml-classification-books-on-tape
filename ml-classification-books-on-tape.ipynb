{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Books on Tape\n",
    "\n",
    "In this project, we will attempt to classify audio books written by classic authors, Jane Austen and Charles Dickens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "To begin, upload one each of an audio book by Austen and Dickens in MP3 format to an Amazon S3 bucket. The audio book MP3 files should be per chapter (i.e. each chapter in its own file).\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Next, configure SageMaker Execution Role to access other AWS resources, including:\n",
    "\n",
    "* Audio file bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::487757292854:role/service-role/AmazonSageMaker-ExecutionRole-20190122T171669\n",
      "CPU times: user 813 ms, sys: 201 ms, total: 1.01 s\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "training_image = get_image_uri(boto3.Session().region_name, 'blazingtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation: Transcription\n",
    "\n",
    "Audio books have already been stored in Amazon S3 Bucket. To prepare data for analysis, use Amazon Transcribe to create labelled training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-2-487757292854\n"
     ]
    }
   ],
   "source": [
    "# get the SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# operate in the default SageMaker bucket when generating output of Transcribe job\n",
    "sm_bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(sm_bucket)\n",
    "prefix = 'transcribe'\n",
    "\n",
    "# identify where the Dickens and Austen book files are held\n",
    "mp3_bucket = 'ml-classification-books-on-tape'\n",
    "austen_book = 'austen_sense_and_sensibility'\n",
    "dickens_book = 'dickens_great_expectations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of Austen and Dickens, we need to grep for the list of MP3 files and start a Transcribe job for each. The output of the Transcribe job should be stored in a properly prefixed output directory in our SageMaker S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the S3 URI for the passed mp3_file\n",
    "def media_file_uri(mp3_file):\n",
    "    return 'https://s3-{}.amazonaws.com/{}/{}'.format(\n",
    "        boto3.Session().region_name,\n",
    "        mp3_bucket,\n",
    "        mp3_file\n",
    "    )\n",
    "\n",
    "# Starts an Amazon Transcribe job for the passed mp3_file\n",
    "def start_transcribe_job(mp3_file):\n",
    "    transcribe = boto3.client('transcribe')\n",
    "    response = transcribe.start_transcription_job(\n",
    "        # need to remove the slash from job name\n",
    "        TranscriptionJobName=mp3_file.replace('/', '--'),\n",
    "        LanguageCode='en-US',\n",
    "        MediaFormat='mp3',\n",
    "        Media={\n",
    "            'MediaFileUri': media_file_uri(mp3_file)\n",
    "        },\n",
    "        OutputBucketName=sm_bucket\n",
    "    )\n",
    "    print('Transcribe job status: {}'.format(response['TranscriptionJob']['TranscriptionJobStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of mp3 files in the specified prefix\n",
    "def get_mp3_objects_for(book):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects(\n",
    "        Bucket=mp3_bucket,\n",
    "        Prefix=book\n",
    "    )\n",
    "    \n",
    "    keys=[]\n",
    "    for obj in response['Contents']:\n",
    "        keys.append(obj['Key'])\n",
    "    \n",
    "    print('Found {} MP3 files in {}'.format(len(keys), book))\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 MP3 files in austen_sense_and_sensibility\n",
      "austen_sense_and_sensibility/senseandsensibility_01_austen_64kb.mp3\n",
      "Transcribe job status: IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "# start with Jane Austen...\n",
    "austen_files = get_mp3_objects_for(austen_book)\n",
    "\n",
    "# TODO: Iterate through all files and run transcribe jobs... this could take awhile\n",
    "print(austen_files[0])\n",
    "start_transcribe_job(austen_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NOTE:** Transcribing all files can take some time. Please pause here until complete!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "After transcribing the various audio files, we need to preprocess the training data into a format that can be consumed by the BlazingText algorithm. Per [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html), a *space-separated tokenized text format* with class labels (prefixed by `__label__`) in the same line as the original sentence is appropriate. Each setence will be on its own line. We'll use the `nltk` library to tokenize the input sentences. Raw data must be retrieved by processing each `.json` file in the session default S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_chapter(author, chapter):\n",
    "    curr_chapter = []\n",
    "    label = '__label__{}'.format(author)\n",
    "    \n",
    "    # tokenize chapter in to sentences\n",
    "    chapter = nltk.sent_tokenize(chapter)\n",
    "    for sent in chapter:\n",
    "        curr_sent = []\n",
    "        curr_sent.append(label)\n",
    "        # then tokenize to words per BlazingText input spec\n",
    "        curr_sent.extend(nltk.word_tokenize(sent.lower()))\n",
    "        curr_chapter.append(curr_sent)\n",
    "        \n",
    "    return curr_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(book, output_file):\n",
    "    transformed_chapters = []\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    files = list(s3.Bucket(sm_bucket).objects.filter(Prefix=book))\n",
    "    \n",
    "    for file in files:\n",
    "        author = file.key[:file.key.index('_')]\n",
    "        data = json.loads(file.get()['Body'].read().decode('utf-8')) # read json string\n",
    "        chapter = data['results']['transcripts'][0]['transcript']\n",
    "        transformed_chapters.append(transform_chapter(author, chapter))\n",
    "       \n",
    "    # randomize and hold out some % of data set for validation\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.2 ms, sys: 0 ns, total: 49.2 ms\n",
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Preparing training dataset\n",
    "\n",
    "preprocess(austen_book, 'books.train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing can take a few minutes. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "# sess.upload_data(path='dbpedia.train', bucket=sm_bucket, key_prefix=train_channel)\n",
    "# sess.upload_data(path='dbpedia.validation', bucket=sm_bucket, key_prefix=validation_channel)\n",
    "\n",
    "# s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "# s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(sm_bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
